#!/bin/bash
#SBATCH -N 1
#SBATCH --time=__time;time;Wallclock Time (duration of job)__
#SBATCH -J spark
#SBATCH -o __JOB_DIR__/spark.%J.out
#SBATCH -e __JOB_DIR__/spark.%J.err
#SBATCH --ntasks-per-node=1
#SBATCH --constraint=intel
#SBATCH -n __nb_tasks;select;# executors;1,2,3,4,8,16,32,64,128,512,1024__
# Print hostname job executed on.
echo
echo "My NODELIST is: $SLURM_NODELIST"
echo
#
module load spark/2.2.1-bin-hadoop2.6
#module load pyspark/2.2.0
#
scontrol show hostnames > hosts
master=`scontrol show hostnames | head -1`
echo $master
MASTER=spark://$master:7077
echo $MASTER
start-master.sh
start-slave.sh $MASTER
date
# launching spark on 1 nodes
#time python pi.py
#time spark-submit pi.py
date
jupyter-labhub --ip="0.0.0.0" --port=__NOTEBOOK_PORT__  --notebook-dir="~/NOTEBOOKS" --debug
